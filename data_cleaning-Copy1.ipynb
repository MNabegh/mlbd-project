{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "legislative-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greatest-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_table('lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv', lineterminator='\\n', warn_bad_lines=True, names=['user', 'timestamp', 'artist-id', 'artist', 'song-id', 'song'])\n",
    "df_profile = pd.read_csv('lastfm-dataset-1K/userid-profile.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=True, skiprows=1, names=['user', 'gender', 'age', 'country', 'signup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "union-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if some songs share a common ID\n",
    "grouped = df[['song-id', 'song']].groupby(['song-id']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aerial-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows with song names containing multiple rows\n",
    "duplicated = df.loc[df['song-id'].isin(grouped[grouped['song'] > 1].index)]\n",
    "duplicated = duplicated.drop(columns=['user', 'timestamp', 'artist', 'artist-id'])\n",
    "containis_extra_rows = duplicated.apply(lambda x: pd.Series({'id': x[0], 'song': x[1], 'flag':'\\n' in x[1]}), axis=1)\n",
    "containis_extra_rows = containis_extra_rows.loc[containis_extra_rows['flag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smoking-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over those 5k rows to get the extra rows and correct the song name\n",
    "for index, row in containis_extra_rows.iterrows():\n",
    "    row_break = row['song'].split('\\n', 1)\n",
    "    song_name = row_break[0]\n",
    "    df.loc[df['song-id'] == row['id'], 'song'] = song_name\n",
    "    \n",
    "    tsv = StringIO(row_break[1])\n",
    "    df_extra = pd.read_csv(tsv, sep=\"\\t\", warn_bad_lines=True, names=['user', 'timestamp', 'artist-id', 'artist', 'song-id', 'song'])\n",
    "    df = df.append(df_extra, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-cloud",
   "metadata": {},
   "source": [
    "### Fix names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "marked-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if some songs share a common ID\n",
    "grouped = df[['song-id', 'song']].groupby(['song-id']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developing-roots",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many ids has more than 1 song name\n",
    "grouped[grouped['song'] > 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "million-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_be_fixed = grouped[grouped['song'] > 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compact-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('song-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "waiting-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_to_fix = df.groupby(df.index)['song'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "satellite-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ids_to_be_fixed, 'song'] = df.loc[ids_to_be_fixed].reset_index()['song-id'].map(lambda x: map_to_fix[x])\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-canal",
   "metadata": {},
   "source": [
    "### Create matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "approved-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_init = df.groupby(['user', 'song']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "streaming-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_init = matrix_init['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wired-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_init = matrix_init.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "embedded-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "social-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PySpark SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create PySpark DataFrame from Pandas\n",
    "# sparkDF=spark.createDataFrame(matrix_init) \n",
    "# sparkDF.printSchema()\n",
    "# sparkDF.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "apparent-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = (sparkDF\n",
    "            .groupby(sparkDF.song)\n",
    "            .pivot(\"user\")\n",
    "            .avg(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stuffed-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file local folder, delimiter by default is ,\n",
    "matrix.write.format('csv').option('header',True).mode('overwrite').option('sep',',').save('lastfm-dataset-1K/my.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "associate-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.repartition(1).write.format('csv').option('header',True).mode('overwrite').option('sep',',').save('lastfm-dataset-1K/my2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "national-absence",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 993 fields in line 5341, saw 994\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-42fc61686351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatrix_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lastfm-dataset-1K/my2.csv/part-00000-9a28ec76-1292-49cd-904a-60a800e91bc2-c000.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Anaconda3/envs/kaggle/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/envs/kaggle/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/envs/kaggle/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/envs/kaggle/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2059\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 993 fields in line 5341, saw 994\n"
     ]
    }
   ],
   "source": [
    "matrix_pd = pd.read_csv('lastfm-dataset-1K/my2.csv/part-00000-9a28ec76-1292-49cd-904a-60a800e91bc2-c000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "reported-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.format(\"csv\").load(\"lastfm-dataset-1K/my.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aware-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.repartition(1).write.parquet(\"lastfm-dataset-1K/my2.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "surgical-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "double-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow = pq.read_table('lastfm-dataset-1K/my2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_pd = arrow.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-croatia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
