{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lyricsgenius as lg\n",
    "import textdistance as td\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprepocessing:\n",
    "Here we make a dataset with all unique song as entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = pd.read_csv(\"lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv\",sep= '\\t',error_bad_lines = False,names = [\"userID\",\"DateTime\",\"MBID_Artistid\",\"Artist\",\"MBID_Songid\",\"Song\"])\n",
    "song = song[[\"MBID_Songid\",\"Song\",\"Artist\"]]\n",
    "song[\"Song_Artist\"] = song.Song +' '+song.Artist\n",
    "songs_count = song.Song_Artist.value_counts()\n",
    "songs_count = pd.DataFrame(songs_count).reset_index().rename(columns = {\"Song_Artist\" : \"Count\", \"index\":\"Song_Artist\"})\n",
    "song = song[~song.duplicated('Song_Artist')] \n",
    "song = pd.merge(song,songs_count, on = \"Song_Artist\")\n",
    "song = song.sort_values(\"Count\", ascending = False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics Scraping\n",
    "\n",
    "Here we query the lyrics using the genius API with the python library LyricsGenius. Since the query does not always return the song we asked, we check using the levenshtein similiraty if the returned song name and artist is similar enough to the one we queried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius = lg.Genius(\"Xa7lZ8kmkbGSE0Pj5rnbcVqMdlNYMDy3QZ5eye0wF8GJzittEMZ-r5BZ_L8fhW72\",skip_non_songs=True, verbose = False)\n",
    "\n",
    "def clean_text(text):\n",
    "    if type(text) == str:\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"remix\",\"\")\n",
    "        text = text.replace(\"radio edit\",\"\")\n",
    "        text = text.replace(\"/\",\"\")\n",
    "        text = ' '.join(text.split())\n",
    "        for pair in [\"()\",\"[]\",\"{}\"]:\n",
    "            start=0\n",
    "            end = 0\n",
    "            while start !=-1 and end != -1 and start <= end and len(text)>0:\n",
    "                start = text.find(pair[0])\n",
    "                end = text.find(pair[1])\n",
    "                if text[0] == pair[0] and text[-1] == pair[1]:\n",
    "                    text = text[1:-1]\n",
    "                    if text == 'unknown':\n",
    "                        text = ''\n",
    "                elif start !=-1 and end != -1:\n",
    "                    text = text[:start] + text[end+1:]\n",
    "        text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def levenshtein_verification(row)\n",
    "    song = row.Song\n",
    "    artist = row.Artist\n",
    "    query = row.Lyrics\n",
    "    song_sim = max(td.levenshtein.normalized_similarity(song.lower(),query[\"title\"].lower()),\n",
    "                   td.levenshtein.normalized_similarity(clean_text(song),query[\"title\"].lower()))\n",
    "    artist_sim = max(td.levenshtein.normalized_similarity(artist.lower(),query[\"artist\"].lower()),\n",
    "                    td.levenshtein.normalized_similarity(clean_text(artist),query[\"artist\"].lower()))\n",
    "    \n",
    "    if (song_sim >0.6) and (artist_sim > 0.6):\n",
    "        row.Lyrics = {'artist':query[\"artist\"], 'title':query[\"title\"], 'lyrics': query['lyrics']}\n",
    "    else:\n",
    "        row.Lyrics = '[unfound]'\n",
    "    return row\n",
    "\n",
    "song['Lyrics'] = ''\n",
    "\n",
    "for ind,row in song.iterrows():\n",
    "    if type(row.Song_Artist) == str:\n",
    "        if row.Lyrics == '':\n",
    "            try:\n",
    "                row.Lyrics = genius.search_song(row.Song,row.Artist).to_dict()\n",
    "                lyrics = levenshtein_verification(row).Lyrics\n",
    "            except:\n",
    "                lyrics = '[unfound]'\n",
    "            song.loc[ind,\"Lyrics\"] = lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "Topic modeling using LDA \\\n",
    "We did not manange to find meaningful topic yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as sp\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "import numpy as np\n",
    "from deep_translator import GoogleTranslator,batch_detection, single_detection\n",
    "import gensim\n",
    "\n",
    "@Language.factory(\"language_detector\")\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector(language_detection_function=None)\n",
    "\n",
    "\n",
    "nlp = sp.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('language_detector')\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "#a function to filter and tokenize the lyrics\n",
    "def tokenize_lyrics(lyric):\n",
    "    global count\n",
    "    #remove bracket ([]) and its content\n",
    "    lyric = lyric.replace('\\n',' ')\n",
    "    pair='[]'\n",
    "    start=0\n",
    "    end = 0\n",
    "    while start !=-1 and end != -1 and start <= end and len(lyric)>0:\n",
    "        start = lyric.find(pair[0])\n",
    "        end = lyric.find(pair[1])\n",
    "        if start !=-1 and end != -1:\n",
    "            lyric = lyric[:start] + lyric[end+1:]\n",
    "            \n",
    "    #remove unwanted character\n",
    "    chars = '*%\"-_/&=#@^~¨$€£'\n",
    "    for char in chars:\n",
    "        lyric = replace(char,' ')\n",
    "    lyric = ' '.join(lyric.split()).lower()\n",
    "    \n",
    "    #translate lyric to english\n",
    "    if len(lyric)>0:\n",
    "        doc = nlp(lyric)\n",
    "        lang = doc._.language\n",
    "        if lang[\"language\"] != 'en' or lang[\"score\"]<0.7:\n",
    "            if lang[\"score\"]<0.7:\n",
    "                try:\n",
    "                    lyric=GoogleTranslator(source='auto', target='en').translate(lyric[:4999])\n",
    "                except:\n",
    "                    print(count)\n",
    "                    print(lyric)\n",
    "            else:\n",
    "                try:\n",
    "                    lyric=GoogleTranslator(source=lang[\"language\"], target='en').translate(lyric[:4999])\n",
    "                except:\n",
    "                    print(count)\n",
    "                    print(lyric)\n",
    "            doc = nlp(lyric)\n",
    "\n",
    "        lyric = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_oov]\n",
    "    else:\n",
    "        lyric = []\n",
    "    count+=1\n",
    "    return lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "lyrics = song.Lyrics[song.Lyrics.apply(lambda x : type(x) == dict)].apply(lambda x : x['lyrics'])\n",
    "lyrics = lyrics.apply(tokenize_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "id2word = gensim.corpora.Dictionary(lyrics)\n",
    "corpus = [id2word.doc2bow(lyric) for lyric in lyrics]\n",
    "num_topics = 5\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                       chunksize = 100,\n",
    "                                       passes=5)\n",
    "\n",
    "doc_lda = lda_model[corpus]\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic visualisation\n",
    "topic_term_dists=lda_model.get_topics()\n",
    "doc_topic_dists = np.array([np.array([pair[1] for pair in topics]+[0]*(num_topics-len(topics))) for topics in doc_lda])\n",
    "doc_topic_dists = doc_topic_dists/np.sum(doc_topic_dists,axis=1)[:, None]\n",
    "doc_lengths = [len(doc) for doc in corpus]\n",
    "term_freq = id2word.cfs\n",
    "term_freq = [term_freq[key] for key in sorted(term_freq)]\n",
    "vocab = [key for key in id2word.token2id]\n",
    "\n",
    "import pyLDAvis# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.prepare(topic_term_dists,\n",
    "                                   doc_topic_dists,\n",
    "                                   doc_lengths,\n",
    "                                   vocab, term_freq)\n",
    "LDAvis_prepared\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
