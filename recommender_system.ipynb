{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "recommender_system.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MNabegh/mlbd-project/blob/master/recommender_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VEXgpNp1Cqi"
      },
      "source": [
        "import os\n",
        "\n",
        "path = 'lastfm-dataset-1K'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "\n",
        "  !wget http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz\n",
        "  !tar -xvzf lastfm-dataset-1K.tar.gz\n"
      ],
      "id": "0VEXgpNp1Cqi",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHPzdYY01CTs",
        "outputId": "9c9d1c97-dd14-450e-9834-3185404ce6f2"
      },
      "source": [
        "!ls"
      ],
      "id": "dHPzdYY01CTs",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t   lastfm-dataset-1K.tar.gz  sample_data\n",
            "lastfm-dataset-1K  ratings_fig.eps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ162BxguWhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc90ea1-1025-422e-f43e-9decbec942a1"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "PQ162BxguWhK",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkLzfmKxv1nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2874d05-f9ef-4aea-8ae5-75c316ecf1c0"
      },
      "source": [
        "!ls drive/MyDrive/data"
      ],
      "id": "IkLzfmKxv1nY",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature_df_with_emb_clean.csv  song_map_reduced.csv\n",
            "feature_df_with_emb.csv        spotify_features_cleaned.csv\n",
            "genres_3d_mapping.json\t       spotify_features.csv\n",
            "genres_map.html\t\t       spotify_uris.csv\n",
            "ratings_fig.eps\t\t       topics_embedded.pkl\n",
            "song_map.csv\t\t       umap.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "legislative-photography"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import scipy.sparse as sp\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.style.use('seaborn')\n",
        "%matplotlib inline\n",
        "\n",
        "import datetime\n",
        "import time \n",
        "import math\n",
        "import os\n",
        "%config Completer.use_jedi = False"
      ],
      "id": "legislative-photography",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6JENn92JzyY"
      },
      "source": [
        "path = \"drive/MyDrive/data/feature_df_with_emb_clean.csv\"\n",
        "\n",
        "feature_df_with_emb = pd.read_csv(path)\n",
        "feature_df_with_emb = feature_df_with_emb.drop(columns=['Unnamed: 0', 'genres', 'genres_emb'])\n",
        "feature_df_with_emb = feature_df_with_emb.dropna()"
      ],
      "id": "y6JENn92JzyY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MPlx151Mo_8"
      },
      "source": [
        "feature_df_with_emb.head()"
      ],
      "id": "9MPlx151Mo_8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbPM69yFJeLO"
      },
      "source": [
        "feature_df_with_emb.isna().sum()"
      ],
      "id": "pbPM69yFJeLO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greatest-invitation"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_table('lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv', lineterminator='\\n', warn_bad_lines=True, names=['user', 'timestamp', 'artist-id', 'artist', 'song-id', 'song'])\n",
        "df_profile = pd.read_csv('lastfm-dataset-1K/userid-profile.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=True, skiprows=1, names=['user', 'gender', 'age', 'country', 'signup'])"
      ],
      "id": "greatest-invitation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "union-season"
      },
      "source": [
        "# Check if some songs share a common ID\n",
        "grouped = df[['song-id', 'song']].groupby(['song-id']).nunique()"
      ],
      "id": "union-season",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aerial-damages"
      },
      "source": [
        "# Get rows with song names containing multiple rows\n",
        "duplicated = df.loc[df['song-id'].isin(grouped[grouped['song'] > 1].index)]\n",
        "duplicated = duplicated.drop(columns=['user', 'timestamp', 'artist', 'artist-id'])\n",
        "containis_extra_rows = duplicated.apply(lambda x: pd.Series({'id': x[0], 'song': x[1], 'flag':'\\n' in x[1]}), axis=1)\n",
        "containis_extra_rows = containis_extra_rows.loc[containis_extra_rows['flag']]"
      ],
      "id": "aerial-damages",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smoking-vanilla"
      },
      "source": [
        "# Iterate over those 5k rows to get the extra rows and correct the song name\n",
        "for index, row in containis_extra_rows.iterrows():\n",
        "    row_break = row['song'].split('\\n', 1)\n",
        "    song_name = row_break[0]\n",
        "    df.loc[df['song-id'] == row['id'], 'song'] = song_name\n",
        "    \n",
        "    tsv = StringIO(row_break[1])\n",
        "    df_extra = pd.read_csv(tsv, sep=\"\\t\", warn_bad_lines=True, names=['user', 'timestamp', 'artist-id', 'artist', 'song-id', 'song'])\n",
        "    df = df.append(df_extra, ignore_index=True)"
      ],
      "id": "smoking-vanilla",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "green-cloud"
      },
      "source": [
        "### Fix names"
      ],
      "id": "green-cloud"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "marked-return"
      },
      "source": [
        "# Check if some songs share a common ID\n",
        "grouped = df[['song-id', 'song']].groupby(['song-id']).nunique()"
      ],
      "id": "marked-return",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "developing-roots"
      },
      "source": [
        "# Check how many ids has more than 1 song name\n",
        "grouped[grouped['song'] > 1].shape[0]"
      ],
      "id": "developing-roots",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "million-deputy"
      },
      "source": [
        "ids_to_be_fixed = grouped[grouped['song'] > 1].index"
      ],
      "id": "million-deputy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compact-standing"
      },
      "source": [
        "df = df.set_index('song-id')"
      ],
      "id": "compact-standing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waiting-efficiency"
      },
      "source": [
        "map_to_fix = df.groupby(df.index)['song'].head(1)"
      ],
      "id": "waiting-efficiency",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "satellite-chile"
      },
      "source": [
        "df.loc[ids_to_be_fixed, 'song'] = df.loc[ids_to_be_fixed].reset_index()['song-id'].map(lambda x: map_to_fix[x])\n",
        "df = df.reset_index()"
      ],
      "id": "satellite-chile",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experimental-canal"
      },
      "source": [
        "### Create matrix"
      ],
      "id": "experimental-canal"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "approved-episode"
      },
      "source": [
        "matrix_init = df.groupby(['user', 'artist', 'song']).count()"
      ],
      "id": "approved-episode",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "streaming-platform"
      },
      "source": [
        "matrix_init = matrix_init['timestamp']\n",
        "matrix_init = matrix_init.map(lambda x: np.log(x))"
      ],
      "id": "streaming-platform",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wired-master"
      },
      "source": [
        "matrix_init = matrix_init.reset_index()"
      ],
      "id": "wired-master",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peaceful-uzbekistan"
      },
      "source": [
        "def rating_scaler(row):\n",
        "    row_array = np.array(row)\n",
        "    lower_bound, new_range= 3, 7\n",
        "    min_, max_ = row_array.min(), row_array.max()\n",
        "    old_range = max_ - min_\n",
        "    \n",
        "    \n",
        "    scaled_row = (new_range * (row_array - min_)) / (old_range + 1e-6)  + lower_bound\n",
        "    scaled_row[scaled_row == 2] = 0\n",
        "    return pd.Series(scaled_row)"
      ],
      "id": "peaceful-uzbekistan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwZGo8Gx4LG8"
      },
      "source": [
        ""
      ],
      "id": "jwZGo8Gx4LG8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sunrise-article"
      },
      "source": [
        "matrix_init_scaled = matrix_init.groupby('user')['timestamp'].apply(rating_scaler)"
      ],
      "id": "sunrise-article",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "promising-sussex"
      },
      "source": [
        "matrix_init['timestamp'] = matrix_init_scaled.reset_index()['timestamp']"
      ],
      "id": "promising-sussex",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xz9agnu2Acj"
      },
      "source": [
        "matrix_init['timestamp'].describe()"
      ],
      "id": "1xz9agnu2Acj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4U2zX1mFFJh"
      },
      "source": [
        "(matrix_init['timestamp'] == 0).sum() / matrix_init.shape[0]"
      ],
      "id": "H4U2zX1mFFJh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RegnknnLcdpr"
      },
      "source": [
        "feature_df_with_emb['item'] = feature_df_with_emb.apply(lambda row: row[1]+' '+row[0], axis=1)"
      ],
      "id": "RegnknnLcdpr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilcbgky8mxyH"
      },
      "source": [
        "topics_df = pd.read_pickle('drive/MyDrive/data/topics_embedded.pkl')\n",
        "topics_df[['topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6']] = pd.DataFrame(topics_df['topics'].tolist(), index= topics_df.index)\n",
        "topics_df = topics_df.drop(columns=['Lyrics', 'Lyrics_en', 'topics'])\n",
        "topics_df = topics_df.rename({'Song_Artist':'item'}, axis=1)"
      ],
      "id": "Ilcbgky8mxyH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj18D6cemzuc"
      },
      "source": [
        "topics_df.head()"
      ],
      "id": "Pj18D6cemzuc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "working-dylan"
      },
      "source": [
        "### Merge data"
      ],
      "id": "working-dylan"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "silent-present"
      },
      "source": [
        "matrix_init = matrix_init.rename({'timestamp':'rating', 'song':'track'}, axis=1)"
      ],
      "id": "silent-present",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PVh-RlLpxEB"
      },
      "source": [
        "matrix_init['rating'].plot.hist()\n",
        "plt.xlabel('Rating')\n",
        "plt.savefig('drive/MyDrive/data/ratings_fig.eps', format='eps')\n",
        "plt.show()"
      ],
      "id": "-PVh-RlLpxEB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "potential-replica"
      },
      "source": [
        "matrix_extended = matrix_init.merge(feature_df_with_emb, on=['artist', 'track'])"
      ],
      "id": "potential-replica",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "formal-token"
      },
      "source": [
        "matrix_extended.head()"
      ],
      "id": "formal-token",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-BObyTK8dft"
      },
      "source": [
        "### Filter the dataset"
      ],
      "id": "w-BObyTK8dft"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmxgAsts8dSB"
      },
      "source": [
        "matrix_extended.groupby('user').count()['rating'].describe()"
      ],
      "id": "bmxgAsts8dSB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVJT_7Qi8cyB"
      },
      "source": [
        "matrix_extended.groupby('user').count()['rating'].plot.hist()\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ],
      "id": "RVJT_7Qi8cyB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9l3wRhoGL4i"
      },
      "source": [
        "matrix_extended.groupby('item').count()['rating'].describe()"
      ],
      "id": "u9l3wRhoGL4i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGlGtGXN8cPj"
      },
      "source": [
        "matrix_extended.groupby('item').count()['rating'].plot.hist()\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ],
      "id": "EGlGtGXN8cPj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3Mpjco08cCg"
      },
      "source": [
        "matrix_extended['rating'].plot.hist()\n",
        "plt.yscale('log')\n",
        "plt.show()Song_Artist"
      ],
      "id": "O3Mpjco08cCg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06x7G_3cGW2J"
      },
      "source": [
        "items_app = matrix_extended.groupby('item').count()['rating']\n",
        "items_app = items_app.loc[items_app > 3.99].index\n",
        "matrix_sampled = matrix_extended.copy()\n",
        "matrix_sampled = matrix_sampled.set_index('item')\n",
        "matrix_sampled = matrix_sampled.loc[items_app]\n",
        "matrix_sampled = matrix_sampled.reset_index()\n",
        "matrix_sampled = matrix_sampled.sample(frac=1)"
      ],
      "id": "06x7G_3cGW2J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9MFL9ZoIB-n"
      },
      "source": [
        "matrix_sampled.groupby('user').count()['rating'].describe()"
      ],
      "id": "x9MFL9ZoIB-n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa1J689p3mOi"
      },
      "source": [
        "matrix_sampled.groupby('item').count()['rating'].describe()"
      ],
      "id": "Fa1J689p3mOi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYWZcgEnyASj"
      },
      "source": [
        "matrix_sampled_merged = matrix_sampled.merge(topics_df, on='item')"
      ],
      "id": "tYWZcgEnyASj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tribal-batch"
      },
      "source": [
        "### Sample Users"
      ],
      "id": "tribal-batch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRKwgOyJdfQb"
      },
      "source": [
        "# no_users = 200\n",
        "# users_list = matrix_sampled.user.unique()\n",
        "# users_sampled = np.random.choice(users_list, replace=False, size=no_users)\n",
        "# matrix_sampled_agg = matrix_sampled.copy()\n",
        "# matrix_sampled_agg = matrix_sampled_agg.set_index('user')\n",
        "# matrix_sampled_agg = matrix_sampled_agg.loc[users_sampled]\n",
        "# matrix_sampled_agg = matrix_sampled_agg.reset_index()\n",
        "# matrix_sampled_agg = matrix_sampled_agg.sample(frac=1)"
      ],
      "id": "YRKwgOyJdfQb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGp8lrRx2raQ"
      },
      "source": [
        "### Sample items\n"
      ],
      "id": "qGp8lrRx2raQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXvEYoqP2kdR"
      },
      "source": [
        "# no_items = 10000\n",
        "# items_app_count = matrix_sampled_agg.groupby('item').count()['rating'].reset_index()\n",
        "# items_list = items_app_count.loc[items_app_count['rating'] > 8, 'item'].values\n",
        "# items_sampled = np.random.choice(items_list, replace=False, size=no_items)\n",
        "# matrix_sampled_agg = matrix_sampled_agg.set_index('item')\n",
        "# matrix_sampled_agg = matrix_sampled_agg.loc[items_sampled]\n",
        "# matrix_sampled_agg = matrix_sampled_agg.reset_index()\n",
        "# matrix_sampled_agg = matrix_sampled_agg.sample(frac=1)"
      ],
      "id": "qXvEYoqP2kdR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlgOyabZiDQ7"
      },
      "source": [
        "matrix_sampled_merged['user_id'] = matrix_sampled_merged['user'].astype('category').cat.codes\n",
        "matrix_sampled_merged['item_id'] = matrix_sampled_merged['item'].astype('category').cat.codes"
      ],
      "id": "GlgOyabZiDQ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJKdudq4uizF"
      },
      "source": [
        "# from random import choices\n",
        "# indices_to_upsample = matrix_sampled_agg.index[matrix_sampled_agg['rating'] > 8].tolist()\n",
        "# indices_upsampled = choices(indices_to_upsample, k=len(indices_to_upsample) * 10)\n",
        "# samples_df = matrix_sampled_agg.loc[indices_upsampled]\n",
        "# matrix_sampled_agg = pd.concat([matrix_sampled_agg, samples_df], axis=0)"
      ],
      "id": "eJKdudq4uizF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyric-crash"
      },
      "source": [
        "X = matrix_sampled_merged[['user_id', \n",
        "              'item_id',\n",
        "              'danceability',\n",
        "              'energy',\n",
        "              'speechiness', \n",
        "              'acousticness', \n",
        "              'instrumentalness', \n",
        "              'liveness',\n",
        "              'valence', \n",
        "              'tempo', \n",
        "              'popularity', \n",
        "              'emb_x', \n",
        "              'emb_y',\n",
        "              'emb_z',\n",
        "              'topic_1',\n",
        "              'topic_2',\n",
        "              'topic_3',\n",
        "              'topic_4',\n",
        "              'topic_5',\n",
        "              'topic_6']]\n",
        "y = matrix_sampled_merged['rating']\n",
        "groups = matrix_sampled_merged['user_id']"
      ],
      "id": "lyric-crash",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "divine-boston"
      },
      "source": [
        "# Change splitting to be by index using https://stackoverflow.com/questions/53490497/getting-validation-set-from-train-set-by-using-percentage-from-groupby-in-pand\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=groups)"
      ],
      "id": "divine-boston",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "organic-irish"
      },
      "source": [
        "no_users, no_items, no_factors = matrix_sampled_merged['user_id'].nunique(), matrix_sampled_merged['item_id'].nunique(), 100"
      ],
      "id": "organic-irish",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzGeolQgIrYM"
      },
      "source": [
        "no_users"
      ],
      "id": "BzGeolQgIrYM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK5iQ0TdIssx"
      },
      "source": [
        "no_items"
      ],
      "id": "oK5iQ0TdIssx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHCn-jhE0tNx"
      },
      "source": [
        "X.shape"
      ],
      "id": "aHCn-jhE0tNx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "super-dynamics"
      },
      "source": [
        "### Models"
      ],
      "id": "super-dynamics"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guilty-louisiana"
      },
      "source": [
        "def create_shallow_model(no_factors, no_users, no_items):\n",
        "    # User branch\n",
        "    user_id = tf.keras.layers.Input(shape=[1], name='user_id')\n",
        "    user_matrix = tf.keras.layers.Embedding(no_users+1, no_factors, name='user_matrix')(user_id)\n",
        "    user_vector = tf.keras.layers.Flatten(name='user_vector')(user_matrix)\n",
        "    # Item branch\n",
        "    item_id = tf.keras.layers.Input(shape=[1], name='item_id')\n",
        "    item_matrix = tf.keras.layers.Embedding(no_items+1, no_factors, name='item_matrix')(item_id)\n",
        "    item_vector = tf.keras.layers.Flatten(name='item_vector')(item_matrix)\n",
        "    # Dot product \n",
        "    vectors_product = tf.keras.layers.dot([user_vector, item_vector], axes=1, normalize=False)\n",
        "    # Model definition\n",
        "    model = tf.keras.models.Model(inputs=[user_id, item_id], outputs=[vectors_product], name='shallow_model')\n",
        "    return model"
      ],
      "id": "guilty-louisiana",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "needed-energy"
      },
      "source": [
        "model = create_shallow_model(no_factors, no_users, no_items)"
      ],
      "id": "needed-energy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "concrete-mediterranean"
      },
      "source": [
        "model.summary()"
      ],
      "id": "concrete-mediterranean",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "million-delaware"
      },
      "source": [
        "def create_deep_model(no_factors, no_users, no_items):\n",
        "    all_input = tf.keras.layers.Input(shape=[20], name='input')\n",
        "    input_reshaped = tf.keras.layers.Reshape((20,1), input_shape=(20,))(all_input)\n",
        "\n",
        "    # Process input\n",
        "    user_id_vec = tf.keras.layers.Cropping1D(cropping=(0,19))(input_reshaped)\n",
        "    user_id = tf.keras.layers.Reshape((1,), input_shape=(1,1))(user_id_vec)\n",
        "\n",
        "    item_id_vec = tf.keras.layers.Cropping1D(cropping=(1,18))(input_reshaped)\n",
        "    item_id = tf.keras.layers.Reshape((1,), input_shape=(1,1))(item_id_vec)\n",
        "    # User branch\n",
        "    user_matrix = tf.keras.layers.Embedding(no_users+1, no_factors, name='user_matrix')(user_id)\n",
        "    user_vector = tf.keras.layers.Flatten(name='user_vector')(user_matrix)\n",
        "    # Item branch\n",
        "    item_matrix = tf.keras.layers.Embedding(no_items+1, no_factors, name='item_matrix')(item_id)\n",
        "    item_vector = tf.keras.layers.Flatten(name='item_vector')(item_matrix)\n",
        "    # Concantenation\n",
        "    vectors_concat = tf.keras.layers.Concatenate()([user_vector, item_vector])\n",
        "    vectors_concat_dropout = tf.keras.layers.Dropout(0.2)(vectors_concat)\n",
        "    # Backbone \n",
        "    dense_1 = tf.keras.layers.Dense(64,name='fc3')(vectors_concat_dropout)\n",
        "    dropout_1 = tf.keras.layers.Dropout(0.2,name='d3')(dense_1)\n",
        "    dense_2 = tf.keras.layers.Dense(32,name='fc4', activation='relu')(dropout_1)\n",
        "    dropout_2 = tf.keras.layers.Dropout(0.2,name='d4')(dense_2)\n",
        "    dense_3 = tf.keras.layers.Dense(16,name='fc5', activation='relu')(dropout_2)\n",
        "    dropout_3 = tf.keras.layers.Dropout(0.2,name='d5')(dense_3)\n",
        "    dense_4 = tf.keras.layers.Dense(8,name='fc6', activation='relu')(dropout_3)\n",
        "    dense_4_output = tf.keras.layers.Dense(1, activation='relu', name='activation')(dense_4)\n",
        "    # Model definition\n",
        "    model = tf.keras.models.Model(inputs=[all_input], outputs=[dense_4_output], name='deep_model')\n",
        "    return model"
      ],
      "id": "million-delaware",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUwPHkkm004I"
      },
      "source": [
        "def create_deep_hybird_model(no_factors, no_users, no_items):\n",
        "    all_input = tf.keras.layers.Input(shape=[20], name='input')\n",
        "    input_reshaped = tf.keras.layers.Reshape((20,1), input_shape=(20,))(all_input)\n",
        "\n",
        "    # Process input\n",
        "    user_id_vec = tf.keras.layers.Cropping1D(cropping=(0,19))(input_reshaped)\n",
        "    user_id = tf.keras.layers.Reshape((1,), input_shape=(1,1))(user_id_vec)\n",
        "\n",
        "    item_id_vec = tf.keras.layers.Cropping1D(cropping=(1,18))(input_reshaped)\n",
        "    item_id = tf.keras.layers.Reshape((1,), input_shape=(1,1))(item_id_vec)\n",
        "\n",
        "    metadata_vec = tf.keras.layers.Cropping1D(cropping=(2,0))(input_reshaped)\n",
        "    metadata = tf.keras.layers.Reshape((18,), input_shape=(18,1))(metadata_vec)\n",
        "\n",
        "    # User branch\n",
        "    user_matrix = tf.keras.layers.Embedding(no_users+1, no_factors, name='user_matrix')(user_id)\n",
        "    user_vector = tf.keras.layers.Flatten(name='user_vector')(user_matrix)\n",
        "    # Item branch\n",
        "    item_matrix = tf.keras.layers.Embedding(no_items+1, no_factors, name='item_matrix')(item_id)\n",
        "    item_vector = tf.keras.layers.Flatten(name='item_vector')(item_matrix)\n",
        "    # Concantenation\n",
        "    vectors_concat = tf.keras.layers.Concatenate()([user_vector, item_vector])\n",
        "    vectors_concat_dropout = tf.keras.layers.Dropout(0.2)(vectors_concat)\n",
        "    # Preprocess metadata\n",
        "    dense_meta_1 = tf.keras.layers.Dense(12,name='fc1')(metadata)\n",
        "    dense_meta_2 = tf.keras.layers.Dense(8,name='fc2')(dense_meta_1)\n",
        "    # Metadata concat\n",
        "    vectors_concat_meta = tf.keras.layers.Concatenate() ([vectors_concat_dropout, dense_meta_2])\n",
        "    # Backbone \n",
        "    dense_1 = tf.keras.layers.Dense(256,name='fc3')(vectors_concat_meta)\n",
        "    dropout_1 = tf.keras.layers.Dropout(0.2,name='d3')(dense_1)\n",
        "    dense_2 = tf.keras.layers.Dense(128,name='fc4', activation='relu')(dropout_1)\n",
        "    dropout_2 = tf.keras.layers.Dropout(0.2,name='d4')(dense_2)\n",
        "    dense_3 = tf.keras.layers.Dense(64,name='fc5', activation='relu')(dropout_2)\n",
        "    dropout_3 = tf.keras.layers.Dropout(0.2,name='d5')(dense_3)\n",
        "    dense_4 = tf.keras.layers.Dense(32,name='fc6', activation='relu')(dropout_3)\n",
        "    dropout_4 = tf.keras.layers.Dropout(0.2,name='d6')(dense_4)\n",
        "    dense_5 = tf.keras.layers.Dense(16,name='fc7', activation='relu')(dropout_4)\n",
        "    dropout_5 = tf.keras.layers.Dropout(0.2,name='d7')(dense_5)\n",
        "    dense_6 = tf.keras.layers.Dense(8,name='fc8', activation='relu')(dropout_5)    \n",
        "    dense_6_output = tf.keras.layers.Dense(1, activation='relu', name='activation')(dense_6)\n",
        "    # Model definition\n",
        "    model = tf.keras.models.Model(inputs=[all_input], outputs=[dense_6_output], name='deep_model')\n",
        "    return model"
      ],
      "id": "TUwPHkkm004I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "canadian-boulder"
      },
      "source": [
        "model = create_deep_model(no_factors, no_users, no_items)"
      ],
      "id": "canadian-boulder",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLjAekay3bs3"
      },
      "source": [
        "model = create_deep_hybird_model(no_factors, no_users, no_items)\n",
        "model.summary()"
      ],
      "id": "ZLjAekay3bs3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "minute-manual"
      },
      "source": [
        "### Model training"
      ],
      "id": "minute-manual"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "confident-conviction"
      },
      "source": [
        "# Input-output data definition\n",
        " \n",
        "# X_train_arr = X_train.values.tolist()\n",
        "# Try a decaying learning rate\n",
        "\n",
        "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=1e-2,\n",
        "#     decay_steps=1500,\n",
        "#     decay_rate=0.9)\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop()#learning_rate=lr_schedule)\n",
        "\n",
        "# Model creation\n",
        "model = create_deep_hybird_model(no_factors, no_users, no_items)\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, min_delta=0.2)\n",
        "\n",
        "# Model compiling \n",
        "model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt)\n",
        "\n",
        "# Model training\n",
        "model.fit(X_train, y_train, epochs=10, callbacks=[callback], batch_size=2048, shuffle=True)"
      ],
      "id": "confident-conviction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "separate-magic"
      },
      "source": [
        "### Prediction"
      ],
      "id": "separate-magic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atlantic-maria"
      },
      "source": [
        "# Predictions in the training set\n",
        "# X_train = [X_train.user_id, X_train.item_id]\n",
        "y_train_pred = model.predict(X_train, batch_size=2048)"
      ],
      "id": "atlantic-maria",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IC4nrL_tcJ6"
      },
      "source": [
        "X_test.head()"
      ],
      "id": "6IC4nrL_tcJ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smoking-apparel"
      },
      "source": [
        "# Predictions in the test set\n",
        "# X_test_arr = [X_test.user_id, X_test.item_id]\n",
        "y_test_pred = model.predict(X_test, batch_size=2048)"
      ],
      "id": "smoking-apparel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "single-gospel"
      },
      "source": [
        "### Evaluation"
      ],
      "id": "single-gospel"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hired-webcam"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print('Train RMSE:', mean_squared_error(y_train.values, y_train_pred, squared=False))\n",
        "print('Test RMSE:', mean_squared_error(y_test.values, y_test_pred, squared=False))"
      ],
      "id": "hired-webcam",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Pljoaoz-jo"
      },
      "source": [
        "def get_test_predictions(model, X_test):\n",
        "  predictions_li = []\n",
        "  for user_id, test_user in tqdm(X_test.groupby('user_id'), position=0, leave=True):    \n",
        "    # Retrieve the unseen items\n",
        "    user_df = test_user[['item_id', 'rating']].copy()\n",
        "    test_pids = user_df['item_id'].values\n",
        "    relevant_pids = set(user_df.loc[user_df['rating'] > 1, 'item_id'].values)\n",
        "    if len(relevant_pids) == 0:\n",
        "      continue\n",
        "    # Make rating predictions for all items for that user\n",
        "    predictions = model.predict(test_user.drop(columns=['rating']), batch_size=2048)\n",
        "    # predictions = pred_func(model, user_id, pid_array, train_ratings)\n",
        "    predictions = predictions.reshape(-1)\n",
        "    irrelevant_pids = set(test_pids).difference(relevant_pids)\n",
        "    predictions_relevant = predictions[user_df['rating'] > 2]\n",
        "    predictions_irrelevant = predictions[user_df['rating'] == 0]\n",
        "    predictions_dict = {'user_id': user_id,\n",
        "                        'predictions': predictions,\n",
        "                        'test_pids': test_pids,\n",
        "                        'relevant_pids': relevant_pids,\n",
        "                        'irrelevant_pids': irrelevant_pids,\n",
        "                        'predictions_relevant': predictions_relevant,\n",
        "                        'predictions_irrelevant': predictions_irrelevant}\n",
        "    predictions_li.append(predictions_dict)\n",
        "\n",
        "  return pd.DataFrame(predictions_li)\n"
      ],
      "id": "K_Pljoaoz-jo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWS4A9h4V2Ws"
      },
      "source": [
        "X_test.head()"
      ],
      "id": "DWS4A9h4V2Ws",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhkj5qFG0HsO"
      },
      "source": [
        "# X_test_arr = np.array(X_test_arr)\n",
        "# test_ratings = pd.DataFrame({'user_id': X_test_arr[0, :], 'item_id':X_test_arr[1, :], 'rating': y_test})\n",
        "X_test_copy = X_test.copy()\n",
        "X_test_copy['rating'] = y_test \n",
        "predictions_df = get_test_predictions(model, X_test_copy)"
      ],
      "id": "Rhkj5qFG0HsO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyGGzKIm3Z-6"
      },
      "source": [
        "def plot_predictions(predictions_df):\n",
        "  pred_arr = [predictions_df[['predictions_relevant']].apply(lambda x: np.hstack(x), axis=0).values.reshape(-1),\n",
        "              predictions_df[['predictions_irrelevant']].apply(lambda x: np.hstack(x), axis=0).values.reshape(-1)]\n",
        "\n",
        "  f, ax = plt.subplots(1, 1)\n",
        "  ax.set_title('Predictions distribution', color='C0')\n",
        "  c = 'red'\n",
        "  bplot = plt.boxplot(pred_arr, labels=['Relevant', 'Irrelevant'], vert=True, patch_artist=True)\n",
        "  # fill with colors\n",
        "  colors = ['C4', 'C2']\n",
        "  for patch, color in zip(bplot['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "\n",
        "  for median in bplot['medians']:\n",
        "    median.set(color='k', linewidth=1.5)\n",
        "\n",
        "  plt.show()"
      ],
      "id": "IyGGzKIm3Z-6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8pqqFPa4PdW"
      },
      "source": [
        "plot_predictions(predictions_df)"
      ],
      "id": "H8pqqFPa4PdW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "honest-globe"
      },
      "source": [
        "def precision_at_k(predictions_df):\n",
        "    precisions = []\n",
        "    # For each user\n",
        "    for row in predictions_df.iterrows():\n",
        "      predictions = row[1][1]\n",
        "      test_pids = row[1][2]\n",
        "      relevant_pids = set(row[1][3])\n",
        "      if len(relevant_pids) == 0:\n",
        "          continue\n",
        "      # Sort the items and het the top k\n",
        "      top_k = predictions > 2\n",
        "      if top_k.sum() == 0:\n",
        "         precisions.append(0)\n",
        "         continue\n",
        "      top_k_items = set(np.array(test_pids)[top_k])\n",
        "      # Compute precision as per definition\n",
        "      precisions.append(len(top_k_items & relevant_pids) / top_k.sum())\n",
        "    return precisions"
      ],
      "id": "honest-globe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asian-divide"
      },
      "source": [
        "precisions = precision_at_k(predictions_df)"
      ],
      "id": "asian-divide",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muslim-sphere"
      },
      "source": [
        "np.mean(precisions), np.std(precisions)"
      ],
      "id": "muslim-sphere",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cosmetic-enhancement"
      },
      "source": [
        "### EXERCISE CELL ###\n",
        "def recall_at_k(predictions_df):\n",
        "    recalls = []\n",
        "    # For each user\n",
        "    for row in predictions_df.iterrows():\n",
        "      predictions = row[1][1]\n",
        "      test_pids = row[1][2]\n",
        "      relevant_pids = set(row[1][3])\n",
        "      if len(relevant_pids) == 0:\n",
        "          continue\n",
        "      # Sort the items and het the top k\n",
        "      top_k = predictions > 2\n",
        "      if top_k.sum() == 0:\n",
        "         recalls.append(0)\n",
        "         continue\n",
        "      top_k_items = set(np.array(test_pids)[top_k])\n",
        "      recalls.append(len(top_k_items & relevant_pids) / len(relevant_pids))\n",
        "    return recalls"
      ],
      "id": "cosmetic-enhancement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "collected-universal"
      },
      "source": [
        "recalls = recall_at_k(predictions_df)"
      ],
      "id": "collected-universal",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hourly-skiing"
      },
      "source": [
        "np.mean(recalls), np.std(recalls)"
      ],
      "id": "hourly-skiing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "answering-drink"
      },
      "source": [
        "### EXERCISE CELL ###\n",
        "def map_at_k(predictions_df, k=100):\n",
        "    maps = []\n",
        "    for row in predictions_df.iterrows():\n",
        "      predictions = row[1][1]\n",
        "      test_pids = row[1][2]\n",
        "      relevant_pids = set(row[1][3])\n",
        "      if len(relevant_pids) == 0:\n",
        "          continue\n",
        "      partial_maps = []\n",
        "      top_k = list(np.argsort(predictions)[:k])\n",
        "      top_k_items = np.array(test_pids)[top_k]\n",
        "      for rank, item_id in enumerate(top_k_items):\n",
        "          if item_id in relevant_pids:\n",
        "              partial_maps.append(len(set(top_k_items[:rank+1]) & relevant_pids) / float(rank+1))\n",
        "      maps.append(.0 if len(partial_maps) == 0 else np.sum(partial_maps) / float(k))\n",
        "    return maps"
      ],
      "id": "answering-drink",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fantastic-sender"
      },
      "source": [
        "maps = map_at_k(predictions_df, k=10)"
      ],
      "id": "fantastic-sender",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "changing-israel"
      },
      "source": [
        "np.mean(maps), np.std(maps)"
      ],
      "id": "changing-israel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "least-brooklyn"
      },
      "source": [
        ""
      ],
      "id": "least-brooklyn",
      "execution_count": null,
      "outputs": []
    }
  ]
}