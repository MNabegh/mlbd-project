{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "legislative-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import time \n",
    "import math\n",
    "import os\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greatest-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_table('lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv', lineterminator='\\n', warn_bad_lines=True, names=['user', 'timestamp', 'artist-id', 'artist', 'song-id', 'song'])\n",
    "df_profile = pd.read_csv('lastfm-dataset-1K/userid-profile.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=True, skiprows=1, names=['user', 'gender', 'age', 'country', 'signup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "union-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if some songs share a common ID\n",
    "grouped = df[['song-id', 'song']].groupby(['song-id']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aerial-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows with song names containing multiple rows\n",
    "duplicated = df.loc[df['song-id'].isin(grouped[grouped['song'] > 1].index)]\n",
    "duplicated = duplicated.drop(columns=['user', 'timestamp', 'artist', 'artist-id'])\n",
    "containis_extra_rows = duplicated.apply(lambda x: pd.Series({'id': x[0], 'song': x[1], 'flag':'\\n' in x[1]}), axis=1)\n",
    "containis_extra_rows = containis_extra_rows.loc[containis_extra_rows['flag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smoking-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over those 5k rows to get the extra rows and correct the song name\n",
    "for index, row in containis_extra_rows.iterrows():\n",
    "    row_break = row['song'].split('\\n', 1)\n",
    "    song_name = row_break[0]\n",
    "    df.loc[df['song-id'] == row['id'], 'song'] = song_name\n",
    "    \n",
    "    tsv = StringIO(row_break[1])\n",
    "    df_extra = pd.read_csv(tsv, sep=\"\\t\", warn_bad_lines=True, names=['user', 'timestamp', 'artist-id', 'artist', 'song-id', 'song'])\n",
    "    df = df.append(df_extra, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-cloud",
   "metadata": {},
   "source": [
    "### Fix names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "marked-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if some songs share a common ID\n",
    "grouped = df[['song-id', 'song']].groupby(['song-id']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developing-roots",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many ids has more than 1 song name\n",
    "grouped[grouped['song'] > 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "million-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_be_fixed = grouped[grouped['song'] > 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compact-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('song-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "waiting-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_to_fix = df.groupby(df.index)['song'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "satellite-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ids_to_be_fixed, 'song'] = df.loc[ids_to_be_fixed].reset_index()['song-id'].map(lambda x: map_to_fix[x])\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-canal",
   "metadata": {},
   "source": [
    "### Create matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "approved-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_init = df.groupby(['user', 'song']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "streaming-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_init = matrix_init['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "wired-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_init = matrix_init.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "peaceful-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_scaler(row):\n",
    "    row_array = np.array(row)\n",
    "    a, new_range= 1, 4\n",
    "    min_, max_ = row_array.min(), row_array.max()\n",
    "    old_range = max_ - min_\n",
    "    \n",
    "    \n",
    "    scaled_row = (new_range * (row_array - min_)) / (old_range + 1e-6)  + a\n",
    "    return pd.Series(scaled_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "sunrise-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_init_scaled = matrix_init.groupby('user')['timestamp'].apply(rating_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "promising-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_init['timestamp'] = matrix_init_scaled.reset_index()['timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-dylan",
   "metadata": {},
   "source": [
    "### Sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "heated-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "british-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_c = CategoricalDtype(sorted(matrix_init.user.unique()), ordered=True)\n",
    "thing_c = CategoricalDtype(sorted(matrix_init.song.unique()), ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "immediate-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = matrix_init.user.astype(person_c).cat.codes\n",
    "col = matrix_init.song.astype(thing_c).cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "silent-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = csr_matrix((matrix_init[\"timestamp\"], (row, col)), \\\n",
    "                           shape=(person_c.categories.size, thing_c.categories.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "potential-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, index=person_c.categories, columns=thing_c.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "formal-token",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>song</th>\n",
       "      <th>rating</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4106948</th>\n",
       "      <td>user_000927</td>\n",
       "      <td>And Some Ya Lose</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>919</td>\n",
       "      <td>72494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4119740</th>\n",
       "      <td>user_000930</td>\n",
       "      <td>Everloving</td>\n",
       "      <td>1.203390</td>\n",
       "      <td>922</td>\n",
       "      <td>291465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920321</th>\n",
       "      <td>user_000210</td>\n",
       "      <td>That'S A Touch I Like</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>209</td>\n",
       "      <td>881354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955355</th>\n",
       "      <td>user_000219</td>\n",
       "      <td>Jagger '67</td>\n",
       "      <td>1.971429</td>\n",
       "      <td>218</td>\n",
       "      <td>467505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142830</th>\n",
       "      <td>user_000503</td>\n",
       "      <td>Creatures</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>496</td>\n",
       "      <td>203968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user                   song    rating  user_id  item_id\n",
       "4106948  user_000927       And Some Ya Lose  1.000000      919    72494\n",
       "4119740  user_000930             Everloving  1.203390      922   291465\n",
       "920321   user_000210  That'S A Touch I Like  1.000000      209   881354\n",
       "955355   user_000219             Jagger '67  1.971429      218   467505\n",
       "2142830  user_000503              Creatures  1.000000      496   203968"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_init.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-batch",
   "metadata": {},
   "source": [
    "### Another format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "generic-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_init['user_id'] = matrix_init['user'].astype('category').cat.codes\n",
    "matrix_init['item_id'] = matrix_init['song'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "european-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_init = matrix_init.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "polyphonic-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_init = matrix_init.rename({'timestamp':'rating'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "lyric-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = matrix_init[['user_id', 'item_id']]\n",
    "y = matrix_init['rating']\n",
    "groups = matrix_init['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "divine-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change splitting to be by index using https://stackoverflow.com/questions/53490497/getting-validation-set-from-train-set-by-using-percentage-from-groupby-in-pand\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "organic-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_users, no_items, no_factors = matrix_init['user_id'].nunique(), matrix_init['item_id'].nunique(), 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-dynamics",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "guilty-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shallow_model(no_factors, no_users, no_items):\n",
    "    # User branch\n",
    "    user_id = tf.keras.layers.Input(shape=[1], name='user_id')\n",
    "    user_matrix = tf.keras.layers.Embedding(no_users+1, no_factors, name='user_matrix')(user_id)\n",
    "    user_vector = tf.keras.layers.Flatten(name='user_vector')(user_matrix)\n",
    "    # Item branch\n",
    "    item_id = tf.keras.layers.Input(shape=[1], name='item_id')\n",
    "    item_matrix = tf.keras.layers.Embedding(no_items+1, no_factors, name='item_matrix')(item_id)\n",
    "    item_vector = tf.keras.layers.Flatten(name='item_vector')(item_matrix)\n",
    "    # Dot product \n",
    "    vectors_product = tf.keras.layers.dot([user_vector, item_vector], axes=1, normalize=False)\n",
    "    # Model definition\n",
    "    model = tf.keras.models.Model(inputs=[user_id, item_id], outputs=[vectors_product], name='shallow_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "needed-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_shallow_model(no_factors, no_users, no_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "concrete-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"shallow_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_matrix (Embedding)         (None, 1, 100)       99300       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "item_matrix (Embedding)         (None, 1, 100)       108392400   item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "user_vector (Flatten)           (None, 100)          0           user_matrix[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "item_vector (Flatten)           (None, 100)          0           item_matrix[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           user_vector[0][0]                \n",
      "                                                                 item_vector[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 108,491,700\n",
      "Trainable params: 108,491,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "million-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_model(no_factors, no_users, no_items):\n",
    "    # User branch\n",
    "    user_id = tf.keras.layers.Input(shape=[1], name='user_id')\n",
    "    user_matrix = tf.keras.layers.Embedding(no_users+1, no_factors, name='user_matrix')(user_id)\n",
    "    user_vector = tf.keras.layers.Flatten(name='user_vector')(user_matrix)\n",
    "    # Item branch\n",
    "    item_id = tf.keras.layers.Input(shape=[1], name='item_id')\n",
    "    item_matrix = tf.keras.layers.Embedding(no_items+1, no_factors, name='item_matrix')(item_id)\n",
    "    item_vector = tf.keras.layers.Flatten(name='item_vector')(item_matrix)\n",
    "    # Concantenation\n",
    "    vectors_concat = tf.keras.layers.Concatenate()([user_vector, item_vector])\n",
    "    vectors_concat_dropout = tf.keras.layers.Dropout(0.2)(vectors_concat)\n",
    "    # Backbone \n",
    "    dense_1 = tf.keras.layers.Dense(16,name='fc3')(vectors_concat_dropout)\n",
    "    dropout_1 = tf.keras.layers.Dropout(0.2,name='d3')(dense_1)\n",
    "    dense_2 = tf.keras.layers.Dense(8,name='fc4', activation='relu')(dropout_1)\n",
    "    dense_2_output = tf.keras.layers.Dense(1, activation='relu', name='activation')(dense_2)\n",
    "    # Model definition\n",
    "    model = tf.keras.models.Model(inputs=[user_id, item_id], outputs=[dense_2_output], name='deep_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "canadian-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_deep_model(no_factors, no_users, no_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-manual",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "sufficient-brook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1615633    1.000000\n",
       "2202736    1.358621\n",
       "262981     1.000000\n",
       "1212865    1.000000\n",
       "2661981    1.000000\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "confident-conviction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3527750 samples\n",
      "Epoch 1/10\n",
      "3527750/3527750 [==============================] - 562s 159us/sample - loss: 0.9691\n",
      "Epoch 2/10\n",
      "3527750/3527750 [==============================] - 496s 141us/sample - loss: 0.3702\n",
      "Epoch 3/10\n",
      "3527750/3527750 [==============================] - 476s 135us/sample - loss: 0.2204\n",
      "Epoch 4/10\n",
      "3527750/3527750 [==============================] - 488s 138us/sample - loss: 0.1515\n",
      "Epoch 5/10\n",
      "3527750/3527750 [==============================] - 491s 139us/sample - loss: 0.1164\n",
      "Epoch 6/10\n",
      "3527750/3527750 [==============================] - 476s 135us/sample - loss: 0.1004\n",
      "Epoch 7/10\n",
      "3527750/3527750 [==============================] - 471s 134us/sample - loss: 0.0931\n",
      "Epoch 8/10\n",
      "3527750/3527750 [==============================] - 476s 135us/sample - loss: 0.0884\n",
      "Epoch 9/10\n",
      "3527750/3527750 [==============================] - 470s 133us/sample - loss: 0.0848\n",
      "Epoch 10/10\n",
      "3527750/3527750 [==============================] - 461s 131us/sample - loss: 0.0811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f99d542b410>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input-output data definition\n",
    "X_train = [X_train.user_id, X_train.item_id]\n",
    "\n",
    "# Model creation\n",
    "model = create_shallow_model(no_factors, no_users, no_items)\n",
    "\n",
    "# Model compiling \n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "# Model training\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=2048, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-magic",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "atlantic-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions in the training set\n",
    "# X_train = [X_train.user_id, X_train.item_id]\n",
    "y_train_pred = model.predict(X_train, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "smoking-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions in the test set\n",
    "X_test = [X_test.user_id, X_test.item_id]\n",
    "y_test_pred = model.predict(X_test, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-gospel",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "hired-webcam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.27254927709385485\n",
      "Test RMSE: 0.5599868428679718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print('Train RMSE:', mean_squared_error(y_train.values, y_train_pred, squared=False))\n",
    "print('Test RMSE:', mean_squared_error(y_test.values, y_test_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "written-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_latent(model, uid, pids, train_ratings=None):\n",
    "    user_vector = model.get_layer('user_matrix').get_weights()[0][uid]\n",
    "    item_vectors = model.get_layer('item_matrix').get_weights()[0][pids]\n",
    "    scores = (np.dot(user_vector, item_vectors.T))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "honest-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(model, pred_func, train_ratings, test_ratings, no_users, no_items, k=10):\n",
    "    pid_array = np.arange(no_items, dtype=np.int32)\n",
    "    precisions = []\n",
    "    # For each user\n",
    "    for user_id, user_test_rating in tqdm(test_ratings.groupby('user_id')):\n",
    "        # Retrieve already-seen items\n",
    "        train_pids = train_ratings[train_ratings['user_id'] == user_id]['item_id'].values\n",
    "        # Retrieve the unseen items\n",
    "        test_pids = set(user_test_rating['item_id'].values)\n",
    "        # Make rating predictions for all items for that user\n",
    "        predictions = pred_func(model, user_id, pid_array, train_ratings)\n",
    "        # Force a low rating to already-seen items\n",
    "        predictions[train_pids] = - math.inf\n",
    "        # Sort the items and het the top k\n",
    "        top_k = set(np.argsort(-predictions)[:k])\n",
    "        # Compute precision as per definition\n",
    "        precisions.append(len(top_k & test_pids) / float(k))\n",
    "    return precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "committed-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "modern-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings = pd.DataFrame({'user_id': X_train[0, :], 'item_id':X_train[1, :], 'rating': y_train})\n",
    "test_ratings = pd.DataFrame({'user_id': X_test[0, :], 'item_id':X_test[1, :], 'rating': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "asian-divide",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 991/991 [08:23<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "precisions = precision_at_k(model, predict_from_latent, train_ratings, test_ratings, no_users, no_items, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "muslim-sphere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0009081735620585267, 0.009486441745303166)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precisions), np.std(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cosmetic-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ###\n",
    "def recall_at_k(model, pred_func, train_ratings, test_ratings, no_users, no_items, k=10):\n",
    "    pid_array = np.arange(no_items, dtype=np.int32)\n",
    "    recalls = []\n",
    "    for user_id, user_test_rating in tqdm(test_ratings.groupby('user_id')):\n",
    "        train_pids = train_ratings[train_ratings['user_id'] == user_id]['item_id'].values\n",
    "        test_pids = set(user_test_rating['item_id'].values)\n",
    "        predictions = pred_func(model, user_id, pid_array, train_ratings)\n",
    "        predictions[train_pids] = - math.inf\n",
    "        top_k = set(np.argsort(-predictions)[:k])\n",
    "        recalls.append(len(top_k & test_pids) / len(test_pids))\n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "collected-universal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 991/991 [08:04<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "recalls = recall_at_k(model, predict_from_latent, train_ratings, test_ratings, no_users, no_items, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "hourly-skiing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.66411002895107e-06, 0.00013890930996086155)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(recalls), np.std(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "answering-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ###\n",
    "def map_at_k(model, pred_func, train_ratings, test_ratings, no_users, no_items, k=10):\n",
    "    pid_array = np.arange(no_items, dtype=np.int32)\n",
    "    maps = []\n",
    "    for user_id, user_test_rating in tqdm(test_ratings.groupby('user_id')):\n",
    "        train_pids = train_ratings[train_ratings['user_id'] == user_id]['item_id'].values\n",
    "        test_pids = set(user_test_rating['item_id'].values)\n",
    "        predictions = pred_func(model, user_id, pid_array, train_ratings)\n",
    "        predictions[train_pids] = - math.inf\n",
    "        partial_maps = []\n",
    "        top_k = list(np.argsort(-predictions)[:k])\n",
    "        for rank, item_id in enumerate(top_k):\n",
    "            if item_id in test_pids:\n",
    "                partial_maps.append(len(set(top_k[:rank+1]) & test_pids) / float(rank+1))\n",
    "        maps.append(.0 if len(partial_maps) == 0 else np.sum(partial_maps) / float(k))\n",
    "    return maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fantastic-sender",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 991/991 [07:17<00:00,  2.27it/s]\n"
     ]
    }
   ],
   "source": [
    "maps = map_at_k(model, predict_from_latent, train_ratings, test_ratings, no_users, no_items, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "changing-israel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.000279299408966412, 0.0040065568794788375)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(maps), np.std(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
